{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65k06HwcP3ho"
      },
      "outputs": [],
      "source": [
        "# if using colab run this\n",
        "!git clone https://github.com/HAVIGILI/Anomaly_understanding.git\n",
        "%cd Anomaly_understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeOn0eMyP3hu"
      },
      "outputs": [],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hT8XbMt-P3hv"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "validation_transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, transform=train_transform, download=True)\n",
        "validation_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=validation_transform, download=True)\n",
        "print(\"length of train data\", len(train_dataset), \"length of validation data\", len(validation_dataset), \"nr of classes\", len(train_dataset.classes))\n",
        "\n",
        "cifar_train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "cifar_validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Cw6M0ftP3hx"
      },
      "outputs": [],
      "source": [
        "#import torchvision\n",
        "#from custommodel import CustomModel\n",
        "\n",
        "#cnn_model = CustomModel(32, len(train_dataset.classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naPPnu8QP3hy"
      },
      "outputs": [],
      "source": [
        "#from trainer import Trainer\n",
        "\n",
        "#ModelTrainer = Trainer(cnn_model, cifar_train_loader, cifar_validation_loader, 10, 0.001)\n",
        "#ModelTrainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_PmTxbgP3hz"
      },
      "outputs": [],
      "source": [
        "from pytorch_ood.model import WideResNet\n",
        "from torchvision import datasets\n",
        "from pytorch_ood.utils import ToUnknown\n",
        "import random\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "trans = WideResNet.transform_for(\"cifar10-pt\")\n",
        "\n",
        "class ToTensorTarget:\n",
        "    def __call__(self, y):\n",
        "        return torch.tensor(y)\n",
        "\n",
        "import torchvision.transforms as T\n",
        "\n",
        "target_transform = T.Compose([\n",
        "    ToUnknown(),\n",
        "    ToTensorTarget()\n",
        "])\n",
        "\n",
        "ood_dataset1 = datasets.SVHN(root=\"data\", split=\"test\", transform=trans, download=True, target_transform=ToUnknown())\n",
        "ood_dataset2 = datasets.FakeData(size=10000, image_size=(3, 32, 32), num_classes=10, transform=trans, target_transform=target_transform)\n",
        "ood_dataset3 = datasets.MNIST(root=\"data\", train=False, transform=trans, download=True, target_transform=ToUnknown())\n",
        "ood_dataset4 = datasets.CIFAR100(root=\"data\", train=False, transform=trans, download=True, target_transform=ToUnknown())\n",
        "ood_dataset5 = datasets.FashionMNIST(root=\"data\", train=False, transform=trans, download=True, target_transform=ToUnknown())\n",
        "ood_dataset6 = datasets.STL10(root=\"data\", split=\"test\", transform=trans, download=True, target_transform=ToUnknown())\n",
        "\n",
        "id_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=validation_transform, download=True)\n",
        "\n",
        "ood_datasets = [ood_dataset1, ood_dataset2, ood_dataset3, ood_dataset4, ood_dataset5, ood_dataset6]  # Add all your OOD datasets here\n",
        "\n",
        "desired_size = 5000  # Set your desired size\n",
        "\n",
        "for i, ood_dataset in enumerate(ood_datasets):\n",
        "    all_indices = list(range(len(ood_dataset)))\n",
        "    random.shuffle(all_indices)\n",
        "    subset_indices = all_indices[:desired_size]\n",
        "    subset = Subset(ood_dataset, subset_indices)\n",
        "    ood_datasets[i] = subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEsV4H-AP3h0"
      },
      "outputs": [],
      "source": [
        "from anomaly_detector import AnomalyDetector\n",
        "\n",
        "model = WideResNet(num_classes=10, pretrained=\"cifar10-pt\").to(device).eval()\n",
        "calibration_dataset = train_dataset\n",
        "id_test_dataset = id_dataset\n",
        "\n",
        "for i, ood_dataset in enumerate(ood_datasets):\n",
        "    print(\"OOD dataset\", i)\n",
        "    ood_test_dataset = ood_dataset\n",
        "    detector = AnomalyDetector(model, calibration_dataset, id_test_dataset, ood_test_dataset, device=device)\n",
        "\n",
        "    layers = [model.conv1, model.block1, model.block2, model.block3, nn.Sequential(model.bn1, model.relu)]\n",
        "    head = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten(), model.fc)\n",
        "\n",
        "    print(\"multimahalanobis distance results:\")\n",
        "    detector.multimahalanobis(layers, True)\n",
        "\n",
        "    print(\"mahalanobis distance results:\")\n",
        "    detector.mahalanobis(True)\n",
        "\n",
        "    # print(\"openmax results:\")\n",
        "    # detector.openmax(True)\n",
        "\n",
        "    # print(\"gram results:\")\n",
        "    # detector.gram(head, layers, 10, [1, 2, 3, 4, 5], True)\n",
        "\n",
        "    # print(\"maxsoftmax results:\")\n",
        "    # detector.maxsoftmax(True)\n",
        "\n",
        "    # print(\"mcd results:\")\n",
        "    # detector.mcd(30, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--qDAD13P3h1"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# URL of the plane image\n",
        "url = \"https://th.bing.com/th/id/R.21621d8860f8aa6040a48c551a930de2?rik=3uoSLAbD6Voriw&riu=http%3a%2f%2fjamsdesignsinc.com%2fwp-content%2fuploads%2f2018%2f06%2fAirplane_01-square-1024x1024.jpg&ehk=n5hvZiqC3bgBZZ3z9tNUuH%2fBdsLAQFf%2bb2atiLN4Vx0%3d&risl=&pid=ImgRaw&r=0\"\n",
        "\n",
        "# Download the image\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    with open(\"plane.jpg\", \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    print(\"Image downloaded: plane.jpg\")\n",
        "else:\n",
        "    print(\"Failed to download image\")\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# Load the downloaded image\n",
        "img = Image.open(\"plane.jpg\")\n",
        "\n",
        "# Define CIFAR-10 preprocessing: resize, convert to tensor, and normalize\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),  # Resize to 32x32 pixels\n",
        "    transforms.ToTensor(),       # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize (CIFAR-10 stats)\n",
        "])\n",
        "\n",
        "# Preprocess the image\n",
        "input_tensor = preprocess(img).unsqueeze(0)  # Add batch dimension\n",
        "print(\"Preprocessed image tensor:\", input_tensor.shape)\n",
        "\n",
        "# Load your trained model\n",
        "cnn_model.eval()  # Ensure the model is in evaluation mode\n",
        "\n",
        "# Pass the image through the model\n",
        "with torch.no_grad():\n",
        "    output = cnn_model(input_tensor.to(device))\n",
        "    predicted_class = output.argmax(dim=1).item()\n",
        "\n",
        "# CIFAR-10 class labels\n",
        "class_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Print the prediction\n",
        "print(\"Predicted class:\", class_labels[predicted_class])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}