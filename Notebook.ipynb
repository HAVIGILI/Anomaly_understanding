{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/hvgl/.local/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /home/hvgl/.local/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (3.9.2)\n",
      "Requirement already satisfied: pillow in /home/hvgl/.local/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (10.4.0)\n",
      "Requirement already satisfied: torch in /home/hvgl/.local/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (2.4.1)\n",
      "Requirement already satisfied: torchvision in /home/hvgl/.local/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.19.1)\n",
      "Requirement already satisfied: pytorch-ood in /home/hvgl/.local/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.4.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/hvgl/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (4.53.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/hvgl/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/hvgl/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (24.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/hvgl/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/hvgl/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/hvgl/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (2.20.5)\n",
      "Requirement already satisfied: fsspec in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (2024.9.0)\n",
      "Requirement already satisfied: filelock in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (3.16.1)\n",
      "Requirement already satisfied: networkx in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (3.3)\n",
      "Requirement already satisfied: sympy in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (1.13.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/hvgl/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 4)) (12.6.68)\n",
      "Requirement already satisfied: torchmetrics>=1.0.0 in /home/hvgl/.local/lib/python3.10/site-packages (from pytorch-ood->-r requirements.txt (line 6)) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/hvgl/.local/lib/python3.10/site-packages (from pytorch-ood->-r requirements.txt (line 6)) (1.14.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/hvgl/.local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /home/hvgl/.local/lib/python3.10/site-packages (from torchmetrics>=1.0.0->pytorch-ood->-r requirements.txt (line 6)) (0.11.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hvgl/.local/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 4)) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/hvgl/.local/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from lightning-utilities>=0.8.0->torchmetrics>=1.0.0->pytorch-ood->-r requirements.txt (line 6)) (59.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n",
      "length of train data 50000 length of validation data 10000 nr of classes 10\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "validation_transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "train_dataset = CIFAR10(root=\"./data\", train=True, transform=train_transform, download=True)\n",
    "validation_dataset = CIFAR10(root=\"./data\", train=False, transform=validation_transform, download=True)\n",
    "print(train_dataset)\n",
    "print(\"length of train data\", len(train_dataset), \"length of validation data\", len(validation_dataset), \"nr of classes\", len(train_dataset.classes))\n",
    "\n",
    "cifar_train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "cifar_validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custommodel import CustomModel\n",
    "\n",
    "cnn_model = CustomModel(32, len(train_dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/1563], Loss: 2.1426\n",
      "Epoch [1/10], Step [200/1563], Loss: 1.9223\n",
      "Epoch [1/10], Step [300/1563], Loss: 1.7796\n",
      "Epoch [1/10], Step [400/1563], Loss: 1.6943\n",
      "Epoch [1/10], Step [500/1563], Loss: 1.6279\n",
      "Epoch [1/10], Step [600/1563], Loss: 1.6276\n",
      "Epoch [1/10], Step [700/1563], Loss: 1.5591\n",
      "Epoch [1/10], Step [800/1563], Loss: 1.5394\n",
      "Epoch [1/10], Step [900/1563], Loss: 1.5227\n",
      "Epoch [1/10], Step [1000/1563], Loss: 1.5130\n",
      "Epoch [1/10], Step [1100/1563], Loss: 1.4893\n",
      "Epoch [1/10], Step [1200/1563], Loss: 1.4724\n",
      "Epoch [1/10], Step [1300/1563], Loss: 1.4413\n",
      "Epoch [1/10], Step [1400/1563], Loss: 1.4307\n",
      "Epoch [1/10], Step [1500/1563], Loss: 1.4422\n",
      "Epoch [1/10], Accuracy on test set: 49.64%\n",
      "Epoch [2/10], Step [100/1563], Loss: 1.3840\n",
      "Epoch [2/10], Step [200/1563], Loss: 1.3976\n",
      "Epoch [2/10], Step [300/1563], Loss: 1.3334\n",
      "Epoch [2/10], Step [400/1563], Loss: 1.3640\n",
      "Epoch [2/10], Step [500/1563], Loss: 1.3506\n",
      "Epoch [2/10], Step [600/1563], Loss: 1.3200\n",
      "Epoch [2/10], Step [700/1563], Loss: 1.3326\n",
      "Epoch [2/10], Step [800/1563], Loss: 1.3342\n",
      "Epoch [2/10], Step [900/1563], Loss: 1.3044\n",
      "Epoch [2/10], Step [1000/1563], Loss: 1.2877\n",
      "Epoch [2/10], Step [1100/1563], Loss: 1.3150\n",
      "Epoch [2/10], Step [1200/1563], Loss: 1.2935\n",
      "Epoch [2/10], Step [1300/1563], Loss: 1.2937\n",
      "Epoch [2/10], Step [1400/1563], Loss: 1.2661\n",
      "Epoch [2/10], Step [1500/1563], Loss: 1.2936\n",
      "Epoch [2/10], Accuracy on test set: 53.80%\n",
      "Epoch [3/10], Step [100/1563], Loss: 1.2635\n",
      "Epoch [3/10], Step [200/1563], Loss: 1.2565\n",
      "Epoch [3/10], Step [300/1563], Loss: 1.2459\n",
      "Epoch [3/10], Step [400/1563], Loss: 1.2441\n",
      "Epoch [3/10], Step [500/1563], Loss: 1.2536\n",
      "Epoch [3/10], Step [600/1563], Loss: 1.2231\n",
      "Epoch [3/10], Step [700/1563], Loss: 1.2263\n",
      "Epoch [3/10], Step [800/1563], Loss: 1.2318\n",
      "Epoch [3/10], Step [900/1563], Loss: 1.2058\n",
      "Epoch [3/10], Step [1000/1563], Loss: 1.2236\n",
      "Epoch [3/10], Step [1100/1563], Loss: 1.1926\n",
      "Epoch [3/10], Step [1200/1563], Loss: 1.2353\n",
      "Epoch [3/10], Step [1300/1563], Loss: 1.2105\n",
      "Epoch [3/10], Step [1400/1563], Loss: 1.2162\n",
      "Epoch [3/10], Step [1500/1563], Loss: 1.2174\n",
      "Epoch [3/10], Accuracy on test set: 56.51%\n",
      "Epoch [4/10], Step [100/1563], Loss: 1.1690\n",
      "Epoch [4/10], Step [200/1563], Loss: 1.1828\n",
      "Epoch [4/10], Step [300/1563], Loss: 1.1714\n",
      "Epoch [4/10], Step [400/1563], Loss: 1.1779\n",
      "Epoch [4/10], Step [500/1563], Loss: 1.1721\n",
      "Epoch [4/10], Step [600/1563], Loss: 1.2038\n",
      "Epoch [4/10], Step [700/1563], Loss: 1.1760\n",
      "Epoch [4/10], Step [800/1563], Loss: 1.1804\n",
      "Epoch [4/10], Step [900/1563], Loss: 1.1549\n",
      "Epoch [4/10], Step [1000/1563], Loss: 1.1705\n",
      "Epoch [4/10], Step [1100/1563], Loss: 1.2202\n",
      "Epoch [4/10], Step [1200/1563], Loss: 1.1089\n",
      "Epoch [4/10], Step [1300/1563], Loss: 1.1283\n",
      "Epoch [4/10], Step [1400/1563], Loss: 1.1305\n",
      "Epoch [4/10], Step [1500/1563], Loss: 1.1655\n",
      "Epoch [4/10], Accuracy on test set: 59.95%\n",
      "Epoch [5/10], Step [100/1563], Loss: 1.1091\n",
      "Epoch [5/10], Step [200/1563], Loss: 1.1252\n",
      "Epoch [5/10], Step [300/1563], Loss: 1.1264\n",
      "Epoch [5/10], Step [400/1563], Loss: 1.1520\n",
      "Epoch [5/10], Step [500/1563], Loss: 1.1249\n",
      "Epoch [5/10], Step [600/1563], Loss: 1.1761\n",
      "Epoch [5/10], Step [700/1563], Loss: 1.1495\n",
      "Epoch [5/10], Step [800/1563], Loss: 1.1625\n",
      "Epoch [5/10], Step [900/1563], Loss: 1.1187\n",
      "Epoch [5/10], Step [1000/1563], Loss: 1.1320\n",
      "Epoch [5/10], Step [1100/1563], Loss: 1.1423\n",
      "Epoch [5/10], Step [1200/1563], Loss: 1.1082\n",
      "Epoch [5/10], Step [1300/1563], Loss: 1.1347\n",
      "Epoch [5/10], Step [1400/1563], Loss: 1.0952\n",
      "Epoch [5/10], Step [1500/1563], Loss: 1.0951\n",
      "Epoch [5/10], Accuracy on test set: 60.24%\n",
      "Epoch [6/10], Step [100/1563], Loss: 1.0863\n",
      "Epoch [6/10], Step [200/1563], Loss: 1.0846\n",
      "Epoch [6/10], Step [300/1563], Loss: 1.0994\n",
      "Epoch [6/10], Step [400/1563], Loss: 1.0810\n",
      "Epoch [6/10], Step [500/1563], Loss: 1.0720\n",
      "Epoch [6/10], Step [600/1563], Loss: 1.0975\n",
      "Epoch [6/10], Step [700/1563], Loss: 1.1536\n",
      "Epoch [6/10], Step [800/1563], Loss: 1.1126\n",
      "Epoch [6/10], Step [900/1563], Loss: 1.0811\n",
      "Epoch [6/10], Step [1000/1563], Loss: 1.1056\n",
      "Epoch [6/10], Step [1100/1563], Loss: 1.1427\n",
      "Epoch [6/10], Step [1200/1563], Loss: 1.0747\n",
      "Epoch [6/10], Step [1300/1563], Loss: 1.0972\n",
      "Epoch [6/10], Step [1400/1563], Loss: 1.1143\n",
      "Epoch [6/10], Step [1500/1563], Loss: 1.0876\n",
      "Epoch [6/10], Accuracy on test set: 61.35%\n",
      "Epoch [7/10], Step [100/1563], Loss: 1.0975\n",
      "Epoch [7/10], Step [200/1563], Loss: 1.0877\n",
      "Epoch [7/10], Step [300/1563], Loss: 1.0387\n",
      "Epoch [7/10], Step [400/1563], Loss: 1.0591\n",
      "Epoch [7/10], Step [500/1563], Loss: 1.0712\n",
      "Epoch [7/10], Step [600/1563], Loss: 1.1047\n",
      "Epoch [7/10], Step [700/1563], Loss: 1.0691\n",
      "Epoch [7/10], Step [800/1563], Loss: 1.0850\n",
      "Epoch [7/10], Step [900/1563], Loss: 1.0518\n",
      "Epoch [7/10], Step [1000/1563], Loss: 1.1013\n",
      "Epoch [7/10], Step [1100/1563], Loss: 1.0713\n",
      "Epoch [7/10], Step [1200/1563], Loss: 1.0795\n",
      "Epoch [7/10], Step [1300/1563], Loss: 1.0627\n",
      "Epoch [7/10], Step [1400/1563], Loss: 1.0703\n",
      "Epoch [7/10], Step [1500/1563], Loss: 1.0489\n",
      "Epoch [7/10], Accuracy on test set: 61.39%\n",
      "Epoch [8/10], Step [100/1563], Loss: 1.0560\n",
      "Epoch [8/10], Step [200/1563], Loss: 1.0582\n",
      "Epoch [8/10], Step [300/1563], Loss: 1.0483\n",
      "Epoch [8/10], Step [400/1563], Loss: 1.0551\n",
      "Epoch [8/10], Step [500/1563], Loss: 1.0295\n",
      "Epoch [8/10], Step [600/1563], Loss: 1.0292\n",
      "Epoch [8/10], Step [700/1563], Loss: 1.0671\n",
      "Epoch [8/10], Step [800/1563], Loss: 1.0628\n",
      "Epoch [8/10], Step [900/1563], Loss: 1.0478\n",
      "Epoch [8/10], Step [1000/1563], Loss: 1.0565\n",
      "Epoch [8/10], Step [1100/1563], Loss: 1.0764\n",
      "Epoch [8/10], Step [1200/1563], Loss: 1.0461\n",
      "Epoch [8/10], Step [1300/1563], Loss: 1.0631\n",
      "Epoch [8/10], Step [1400/1563], Loss: 1.0466\n",
      "Epoch [8/10], Step [1500/1563], Loss: 1.0545\n",
      "Epoch [8/10], Accuracy on test set: 62.25%\n",
      "Epoch [9/10], Step [100/1563], Loss: 1.0273\n",
      "Epoch [9/10], Step [200/1563], Loss: 1.0572\n",
      "Epoch [9/10], Step [300/1563], Loss: 1.0503\n",
      "Epoch [9/10], Step [400/1563], Loss: 1.0610\n",
      "Epoch [9/10], Step [500/1563], Loss: 1.0462\n",
      "Epoch [9/10], Step [600/1563], Loss: 1.0175\n",
      "Epoch [9/10], Step [700/1563], Loss: 1.0497\n",
      "Epoch [9/10], Step [800/1563], Loss: 1.0782\n",
      "Epoch [9/10], Step [900/1563], Loss: 1.0355\n",
      "Epoch [9/10], Step [1000/1563], Loss: 1.0397\n",
      "Epoch [9/10], Step [1100/1563], Loss: 1.0565\n",
      "Epoch [9/10], Step [1200/1563], Loss: 1.0312\n",
      "Epoch [9/10], Step [1300/1563], Loss: 1.0217\n",
      "Epoch [9/10], Step [1400/1563], Loss: 1.0155\n",
      "Epoch [9/10], Step [1500/1563], Loss: 1.0190\n",
      "Epoch [9/10], Accuracy on test set: 62.47%\n",
      "Epoch [10/10], Step [100/1563], Loss: 1.0357\n",
      "Epoch [10/10], Step [200/1563], Loss: 1.0735\n",
      "Epoch [10/10], Step [300/1563], Loss: 1.0129\n",
      "Epoch [10/10], Step [400/1563], Loss: 1.0242\n",
      "Epoch [10/10], Step [500/1563], Loss: 1.0067\n",
      "Epoch [10/10], Step [600/1563], Loss: 1.0082\n",
      "Epoch [10/10], Step [700/1563], Loss: 0.9899\n",
      "Epoch [10/10], Step [800/1563], Loss: 1.0474\n",
      "Epoch [10/10], Step [900/1563], Loss: 1.0324\n",
      "Epoch [10/10], Step [1000/1563], Loss: 1.0206\n",
      "Epoch [10/10], Step [1100/1563], Loss: 0.9975\n",
      "Epoch [10/10], Step [1200/1563], Loss: 1.0359\n",
      "Epoch [10/10], Step [1300/1563], Loss: 1.0342\n",
      "Epoch [10/10], Step [1400/1563], Loss: 1.0178\n",
      "Epoch [10/10], Step [1500/1563], Loss: 1.0426\n",
      "Epoch [10/10], Accuracy on test set: 64.06%\n"
     ]
    }
   ],
   "source": [
    "from trainer import Trainer\n",
    "ModelTrainer = Trainer(cnn_model, cifar_train_loader, cifar_validation_loader, 10, 0.001)\n",
    "ModelTrainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/test_32x32.mat\n",
      "Files already downloaded and verified\n",
      "5000\n",
      "128\n",
      "tensor([-1, -1,  1,  1,  2,  3, -1,  4,  2,  9, -1, -1,  9,  4,  7,  7,  1,  6,\n",
      "         9, -1, -1, -1,  8,  8,  2, -1,  4, -1, -1, -1,  2, -1,  5,  0,  3,  0,\n",
      "        -1, -1,  7,  3, -1,  2,  0, -1, -1, -1,  2, -1, -1, -1,  5,  9,  5,  6,\n",
      "        -1,  4, -1,  9,  3,  2,  3,  5,  4,  4,  8, -1,  7,  0,  7,  2,  6, -1,\n",
      "         8,  9, -1, -1, -1, -1, -1, -1, -1,  9,  5, -1,  4,  3,  8, -1, -1, -1,\n",
      "        -1, -1,  0, -1,  6, -1,  7,  7,  4, -1,  7, -1,  8,  4, -1, -1,  5, -1,\n",
      "        -1,  5,  0,  8,  5, -1, -1,  8, -1,  4,  7,  0, -1, -1, -1,  3,  0,  6,\n",
      "        -1, -1])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from pytorch_ood.utils import ToUnknown\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def set_to_0(_):\n",
    "    return 0\n",
    "\n",
    "def set_to_1(_):    \n",
    "    return 1\n",
    "\n",
    "# OOD dataset (e.g. SVHN)\n",
    "ood_dataset = datasets.SVHN(root=\"data\", split=\"test\", transform=validation_transform, download=True, target_transform=ToUnknown())\n",
    "id_dataset = CIFAR10(root=\"./data\", train=False, transform=validation_transform, download=True)\n",
    "\n",
    "desired_size = 5000  # for example, keep only 5k samples\n",
    "\n",
    "# 3. Randomly pick indices for your subset\n",
    "all_indices = list(range(len(ood_dataset)))\n",
    "random.shuffle(all_indices)\n",
    "subset_indices = all_indices[:desired_size]\n",
    "\n",
    "# 4. Wrap the original dataset with Subset\n",
    "svhn_subset = Subset(ood_dataset, subset_indices)\n",
    "print(len(svhn_subset))\n",
    "\n",
    "loader = DataLoader(svhn_subset + id_dataset, batch_size=128, shuffle = True)\n",
    "\n",
    "for images, labels in loader:\n",
    "    print(len(labels))\n",
    "    print(labels)  # Transformed labels (e.g., some are 1000)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.5808442234992981, 'AUTC': 0.5141134262084961, 'AUPR-IN': 0.5124869346618652, 'AUPR-OUT': 0.7144832015037537, 'FPR95TPR': 0.7138000130653381}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_ood.utils import OODMetrics\n",
    "from pytorch_ood.detector import OpenMax, MultiMahalanobis\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Wrap your trained model with OpenMax\n",
    "detector = OpenMax(cnn_model.to(device))\n",
    "cnn_model.eval()\n",
    "\n",
    "# 2. Create OOD metrics collector\n",
    "metrics = OODMetrics()\n",
    "\n",
    "# 3. Fit OpenMax on in-distribution (ID) data\n",
    "detector.fit(cifar_train_loader)  # Make sure this loader has ID samples\n",
    "\n",
    "# 4. Evaluate both ID and OOD\n",
    "with torch.no_grad():\n",
    "    # ---- OOD pass (label=True for each sample) ----\n",
    "    for inputs, id in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        metrics.update(detector(inputs), id)\n",
    "\n",
    "# 5. Now compute the metrics (AUROC, etc.)\n",
    "results = metrics.compute()\n",
    "print(\"OpenMax results\", results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m layer4 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mblock3\n\u001b[1;32m     10\u001b[0m detector \u001b[38;5;241m=\u001b[39m MultiMahalanobis([layer1, layer2, layer3, layer4])\n\u001b[0;32m---> 12\u001b[0m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcifar_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_ood/detector/mmahalanobis.py:95\u001b[0m, in \u001b[0;36mMultiMahalanobis.fit\u001b[0;34m(self, data_loader, device)\u001b[0m\n\u001b[1;32m     93\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel[: layer_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     94\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting for layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m z, y \u001b[38;5;241m=\u001b[39m \u001b[43mextract_feature_avg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mz\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     98\u001b[0m zs\u001b[38;5;241m.\u001b[39mappend(z)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_ood/utils/utils.py:300\u001b[0m, in \u001b[0;36mextract_feature_avg\u001b[0;34m(data_loader, model, device)\u001b[0m\n\u001b[1;32m    297\u001b[0m buffer \u001b[38;5;241m=\u001b[39m TensorBuffer()\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m    301\u001b[0m         x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m    302\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/cifar.py:119\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    116\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pytorch_ood.detector import OpenMax, MultiMahalanobis\n",
    "from pytorch_ood.model import WideResNet\n",
    "\n",
    "model = WideResNet(num_classes=10, pretrained=\"cifar10-pt\").to(device).eval()\n",
    "\n",
    "layer1 = model.conv1\n",
    "layer2 = model.block1\n",
    "layer3 = model.block2\n",
    "layer4 = model.block3\n",
    "detector = MultiMahalanobis([layer1, layer2, layer3, layer4])\n",
    "\n",
    "detector.fit(cifar_train_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "{'AUROC': 0.3223710358142853, 'AUTC': 0.514441192150116, 'AUPR-IN': 0.5470985174179077, 'AUPR-OUT': 0.24841156601905823, 'FPR95TPR': 0.9934999942779541}\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing...\")\n",
    "metrics = OODMetrics()\n",
    "\n",
    "for x, y in loader:\n",
    "    metrics.update(detector(x.to(device)), y)\n",
    "\n",
    "print(metrics.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "id_scores = []\n",
    "ood_scores = []\n",
    "\n",
    "z=0\n",
    "with torch.no_grad():\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        scores = detector(x)\n",
    "        for score, label in zip(scores, y):\n",
    "            z+=1\n",
    "            if label == -1:\n",
    "                ood_scores.append(score.item())\n",
    "            else:\n",
    "                id_scores.append(score.item())\n",
    "print(z)\n",
    "plt.hist(id_scores, bins=50, alpha=0.5, label='ID')\n",
    "plt.hist(ood_scores, bins=50, alpha=0.5, label='OOD')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"OOD Scores Distribution\")\n",
    "plt.xlabel(\"OOD Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image downloaded: plane.jpg\n",
      "Preprocessed image tensor: torch.Size([1, 3, 32, 32])\n",
      "Predicted class: airplane\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the plane image\n",
    "url = \"https://th.bing.com/th/id/R.21621d8860f8aa6040a48c551a930de2?rik=3uoSLAbD6Voriw&riu=http%3a%2f%2fjamsdesignsinc.com%2fwp-content%2fuploads%2f2018%2f06%2fAirplane_01-square-1024x1024.jpg&ehk=n5hvZiqC3bgBZZ3z9tNUuH%2fBdsLAQFf%2bb2atiLN4Vx0%3d&risl=&pid=ImgRaw&r=0\"\n",
    "\n",
    "# Download the image\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(\"plane.jpg\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(\"Image downloaded: plane.jpg\")\n",
    "else:\n",
    "    print(\"Failed to download image\")\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load the downloaded image\n",
    "img = Image.open(\"plane.jpg\")\n",
    "\n",
    "# Define CIFAR-10 preprocessing: resize, convert to tensor, and normalize\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize to 32x32 pixels\n",
    "    transforms.ToTensor(),       # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize (CIFAR-10 stats)\n",
    "])\n",
    "\n",
    "# Preprocess the image\n",
    "input_tensor = preprocess(img).unsqueeze(0)  # Add batch dimension\n",
    "print(\"Preprocessed image tensor:\", input_tensor.shape)\n",
    "\n",
    "# Load your trained model\n",
    "cnn_model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "# Pass the image through the model\n",
    "with torch.no_grad():\n",
    "    output = cnn_model(input_tensor)\n",
    "    predicted_class = output.argmax(dim=1).item()\n",
    "\n",
    "# CIFAR-10 class labels\n",
    "class_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Print the prediction\n",
    "print(\"Predicted class:\", class_labels[predicted_class])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
