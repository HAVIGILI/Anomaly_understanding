{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/hvgl/.local/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /home/hvgl/.local/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (3.9.2)\n",
      "Requirement already satisfied: pillow in /home/hvgl/.local/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (10.4.0)\n",
      "Requirement already satisfied: torch in /home/hvgl/.local/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (2.4.1)\n",
      "Requirement already satisfied: torchvision in /home/hvgl/.local/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.19.1)\n",
      "Requirement already satisfied: pytorch-ood in /home/hvgl/.local/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.4.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/hvgl/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (4.53.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/hvgl/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/hvgl/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (24.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/hvgl/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/hvgl/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/hvgl/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (2.20.5)\n",
      "Requirement already satisfied: fsspec in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (2024.9.0)\n",
      "Requirement already satisfied: filelock in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (3.16.1)\n",
      "Requirement already satisfied: networkx in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (3.3)\n",
      "Requirement already satisfied: sympy in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (1.13.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/hvgl/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 4)) (12.6.68)\n",
      "Requirement already satisfied: torchmetrics>=1.0.0 in /home/hvgl/.local/lib/python3.10/site-packages (from pytorch-ood->-r requirements.txt (line 6)) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/hvgl/.local/lib/python3.10/site-packages (from pytorch-ood->-r requirements.txt (line 6)) (1.14.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/hvgl/.local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /home/hvgl/.local/lib/python3.10/site-packages (from torchmetrics>=1.0.0->pytorch-ood->-r requirements.txt (line 6)) (0.11.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hvgl/.local/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 4)) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/hvgl/.local/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from lightning-utilities>=0.8.0->torchmetrics>=1.0.0->pytorch-ood->-r requirements.txt (line 6)) (59.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# if using colab run this\n",
    "!git clone https://github.com/HAVIGILI/Anomaly_understanding.git\n",
    "%cd Anomaly_understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n",
      "length of train data 50000 length of validation data 10000 nr of classes 10\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "validation_transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "train_dataset = CIFAR10(root=\"./data\", train=True, transform=train_transform, download=True)\n",
    "validation_dataset = CIFAR10(root=\"./data\", train=False, transform=validation_transform, download=True)\n",
    "print(train_dataset)\n",
    "print(\"length of train data\", len(train_dataset), \"length of validation data\", len(validation_dataset), \"nr of classes\", len(train_dataset.classes))\n",
    "\n",
    "cifar_train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "cifar_validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custommodel import CustomModel\n",
    "\n",
    "cnn_model = CustomModel(32, len(train_dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import Trainer\n",
    "ModelTrainer = Trainer(cnn_model, cifar_train_loader, cifar_validation_loader, 10, 0.001)\n",
    "ModelTrainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/test_32x32.mat\n",
      "Files already downloaded and verified\n",
      "5000\n",
      "128\n",
      "tensor([-1, -1,  1,  1,  2,  3, -1,  4,  2,  9, -1, -1,  9,  4,  7,  7,  1,  6,\n",
      "         9, -1, -1, -1,  8,  8,  2, -1,  4, -1, -1, -1,  2, -1,  5,  0,  3,  0,\n",
      "        -1, -1,  7,  3, -1,  2,  0, -1, -1, -1,  2, -1, -1, -1,  5,  9,  5,  6,\n",
      "        -1,  4, -1,  9,  3,  2,  3,  5,  4,  4,  8, -1,  7,  0,  7,  2,  6, -1,\n",
      "         8,  9, -1, -1, -1, -1, -1, -1, -1,  9,  5, -1,  4,  3,  8, -1, -1, -1,\n",
      "        -1, -1,  0, -1,  6, -1,  7,  7,  4, -1,  7, -1,  8,  4, -1, -1,  5, -1,\n",
      "        -1,  5,  0,  8,  5, -1, -1,  8, -1,  4,  7,  0, -1, -1, -1,  3,  0,  6,\n",
      "        -1, -1])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from pytorch_ood.utils import ToUnknown\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# OOD dataset (e.g. SVHN)\n",
    "ood_dataset = datasets.SVHN(root=\"data\", split=\"test\", transform=validation_transform, download=True, target_transform=ToUnknown())\n",
    "id_dataset = CIFAR10(root=\"./data\", train=False, transform=validation_transform, download=True)\n",
    "\n",
    "desired_size = 1000\n",
    "\n",
    "all_indices = list(range(len(ood_dataset)))\n",
    "random.shuffle(all_indices)\n",
    "subset_indices = all_indices[:desired_size]\n",
    "svhn_subset = Subset(ood_dataset, subset_indices)\n",
    "print(len(svhn_subset))\n",
    "\n",
    "loader = DataLoader(svhn_subset + id_dataset, batch_size=128, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m layer4 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mblock3\n\u001b[1;32m     10\u001b[0m detector \u001b[38;5;241m=\u001b[39m MultiMahalanobis([layer1, layer2, layer3, layer4])\n\u001b[0;32m---> 12\u001b[0m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcifar_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_ood/detector/mmahalanobis.py:95\u001b[0m, in \u001b[0;36mMultiMahalanobis.fit\u001b[0;34m(self, data_loader, device)\u001b[0m\n\u001b[1;32m     93\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel[: layer_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     94\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting for layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m z, y \u001b[38;5;241m=\u001b[39m \u001b[43mextract_feature_avg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mz\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     98\u001b[0m zs\u001b[38;5;241m.\u001b[39mappend(z)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_ood/utils/utils.py:300\u001b[0m, in \u001b[0;36mextract_feature_avg\u001b[0;34m(data_loader, model, device)\u001b[0m\n\u001b[1;32m    297\u001b[0m buffer \u001b[38;5;241m=\u001b[39m TensorBuffer()\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m    301\u001b[0m         x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m    302\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/cifar.py:119\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    116\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pytorch_ood.detector import OpenMax, MultiMahalanobis\n",
    "from pytorch_ood.model import WideResNet\n",
    "from anomaly_detector import AnomalyDetector\n",
    "\n",
    "model = WideResNet(num_classes=10, pretrained=\"cifar10-pt\").to(device).eval()\n",
    "calibration_dataset = train_dataset\n",
    "id_test_dataset = id_dataset\n",
    "ood_test_datset = svhn_subset\n",
    "detector = AnomalyDetector(model, calibration_dataset, id_test_dataset, ood_test_dataset, device=device)\n",
    "\n",
    "layer1 = model.conv1\n",
    "layer2 = model.block1\n",
    "layer3 = model.block2\n",
    "layer4 = model.block3\n",
    "layers = [layer1, layer2, layer3, layer4]\n",
    "detector.multimahalanobis(layers, True)\n",
    "detector.mahalanobis()\n",
    "detector.openmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image downloaded: plane.jpg\n",
      "Preprocessed image tensor: torch.Size([1, 3, 32, 32])\n",
      "Predicted class: airplane\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the plane image\n",
    "url = \"https://th.bing.com/th/id/R.21621d8860f8aa6040a48c551a930de2?rik=3uoSLAbD6Voriw&riu=http%3a%2f%2fjamsdesignsinc.com%2fwp-content%2fuploads%2f2018%2f06%2fAirplane_01-square-1024x1024.jpg&ehk=n5hvZiqC3bgBZZ3z9tNUuH%2fBdsLAQFf%2bb2atiLN4Vx0%3d&risl=&pid=ImgRaw&r=0\"\n",
    "\n",
    "# Download the image\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(\"plane.jpg\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(\"Image downloaded: plane.jpg\")\n",
    "else:\n",
    "    print(\"Failed to download image\")\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load the downloaded image\n",
    "img = Image.open(\"plane.jpg\")\n",
    "\n",
    "# Define CIFAR-10 preprocessing: resize, convert to tensor, and normalize\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize to 32x32 pixels\n",
    "    transforms.ToTensor(),       # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize (CIFAR-10 stats)\n",
    "])\n",
    "\n",
    "# Preprocess the image\n",
    "input_tensor = preprocess(img).unsqueeze(0)  # Add batch dimension\n",
    "print(\"Preprocessed image tensor:\", input_tensor.shape)\n",
    "\n",
    "# Load your trained model\n",
    "cnn_model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "# Pass the image through the model\n",
    "with torch.no_grad():\n",
    "    output = cnn_model(input_tensor.to(device))\n",
    "    predicted_class = output.argmax(dim=1).item()\n",
    "\n",
    "# CIFAR-10 class labels\n",
    "class_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Print the prediction\n",
    "print(\"Predicted class:\", class_labels[predicted_class])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
