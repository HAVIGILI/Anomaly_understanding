{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/hvgl/.local/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /home/hvgl/.local/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (3.9.2)\n",
      "Requirement already satisfied: pillow in /home/hvgl/.local/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (10.4.0)\n",
      "Requirement already satisfied: torch in /home/hvgl/.local/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (2.4.1)\n",
      "Requirement already satisfied: torchvision in /home/hvgl/.local/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.19.1)\n",
      "Requirement already satisfied: pytorch-ood in /home/hvgl/.local/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.4.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/hvgl/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (4.53.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/hvgl/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/hvgl/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (24.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/hvgl/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/hvgl/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/hvgl/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (2.20.5)\n",
      "Requirement already satisfied: fsspec in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (2024.9.0)\n",
      "Requirement already satisfied: filelock in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (3.16.1)\n",
      "Requirement already satisfied: networkx in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (3.3)\n",
      "Requirement already satisfied: sympy in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (1.13.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/hvgl/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/hvgl/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 4)) (12.6.68)\n",
      "Requirement already satisfied: torchmetrics>=1.0.0 in /home/hvgl/.local/lib/python3.10/site-packages (from pytorch-ood->-r requirements.txt (line 6)) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/hvgl/.local/lib/python3.10/site-packages (from pytorch-ood->-r requirements.txt (line 6)) (1.14.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/hvgl/.local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /home/hvgl/.local/lib/python3.10/site-packages (from torchmetrics>=1.0.0->pytorch-ood->-r requirements.txt (line 6)) (0.11.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hvgl/.local/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 4)) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/hvgl/.local/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from lightning-utilities>=0.8.0->torchmetrics>=1.0.0->pytorch-ood->-r requirements.txt (line 6)) (59.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n",
      "length of train data 50000 length of validation data 10000 nr of classes 10\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "validation_transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "train_dataset = CIFAR10(root=\"./data\", train=True, transform=train_transform, download=True)\n",
    "validation_dataset = CIFAR10(root=\"./data\", train=False, transform=validation_transform, download=True)\n",
    "print(train_dataset)\n",
    "print(\"length of train data\", len(train_dataset), \"length of validation data\", len(validation_dataset), \"nr of classes\", len(train_dataset.classes))\n",
    "\n",
    "cifar_train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "cifar_validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custommodel import CustomModel\n",
    "\n",
    "cnn_model = CustomModel(32, len(train_dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/1563], Loss: 2.1426\n",
      "Epoch [1/10], Step [200/1563], Loss: 1.9223\n",
      "Epoch [1/10], Step [300/1563], Loss: 1.7796\n",
      "Epoch [1/10], Step [400/1563], Loss: 1.6943\n",
      "Epoch [1/10], Step [500/1563], Loss: 1.6279\n",
      "Epoch [1/10], Step [600/1563], Loss: 1.6276\n",
      "Epoch [1/10], Step [700/1563], Loss: 1.5591\n",
      "Epoch [1/10], Step [800/1563], Loss: 1.5394\n",
      "Epoch [1/10], Step [900/1563], Loss: 1.5227\n",
      "Epoch [1/10], Step [1000/1563], Loss: 1.5130\n",
      "Epoch [1/10], Step [1100/1563], Loss: 1.4893\n",
      "Epoch [1/10], Step [1200/1563], Loss: 1.4724\n",
      "Epoch [1/10], Step [1300/1563], Loss: 1.4413\n",
      "Epoch [1/10], Step [1400/1563], Loss: 1.4307\n",
      "Epoch [1/10], Step [1500/1563], Loss: 1.4422\n",
      "Epoch [1/10], Accuracy on test set: 49.64%\n",
      "Epoch [2/10], Step [100/1563], Loss: 1.3840\n",
      "Epoch [2/10], Step [200/1563], Loss: 1.3976\n",
      "Epoch [2/10], Step [300/1563], Loss: 1.3334\n",
      "Epoch [2/10], Step [400/1563], Loss: 1.3640\n",
      "Epoch [2/10], Step [500/1563], Loss: 1.3506\n",
      "Epoch [2/10], Step [600/1563], Loss: 1.3200\n",
      "Epoch [2/10], Step [700/1563], Loss: 1.3326\n",
      "Epoch [2/10], Step [800/1563], Loss: 1.3342\n",
      "Epoch [2/10], Step [900/1563], Loss: 1.3044\n",
      "Epoch [2/10], Step [1000/1563], Loss: 1.2877\n",
      "Epoch [2/10], Step [1100/1563], Loss: 1.3150\n",
      "Epoch [2/10], Step [1200/1563], Loss: 1.2935\n",
      "Epoch [2/10], Step [1300/1563], Loss: 1.2937\n",
      "Epoch [2/10], Step [1400/1563], Loss: 1.2661\n",
      "Epoch [2/10], Step [1500/1563], Loss: 1.2936\n",
      "Epoch [2/10], Accuracy on test set: 53.80%\n",
      "Epoch [3/10], Step [100/1563], Loss: 1.2635\n",
      "Epoch [3/10], Step [200/1563], Loss: 1.2565\n",
      "Epoch [3/10], Step [300/1563], Loss: 1.2459\n",
      "Epoch [3/10], Step [400/1563], Loss: 1.2441\n",
      "Epoch [3/10], Step [500/1563], Loss: 1.2536\n",
      "Epoch [3/10], Step [600/1563], Loss: 1.2231\n",
      "Epoch [3/10], Step [700/1563], Loss: 1.2263\n",
      "Epoch [3/10], Step [800/1563], Loss: 1.2318\n",
      "Epoch [3/10], Step [900/1563], Loss: 1.2058\n",
      "Epoch [3/10], Step [1000/1563], Loss: 1.2236\n",
      "Epoch [3/10], Step [1100/1563], Loss: 1.1926\n",
      "Epoch [3/10], Step [1200/1563], Loss: 1.2353\n",
      "Epoch [3/10], Step [1300/1563], Loss: 1.2105\n",
      "Epoch [3/10], Step [1400/1563], Loss: 1.2162\n",
      "Epoch [3/10], Step [1500/1563], Loss: 1.2174\n",
      "Epoch [3/10], Accuracy on test set: 56.51%\n",
      "Epoch [4/10], Step [100/1563], Loss: 1.1690\n",
      "Epoch [4/10], Step [200/1563], Loss: 1.1828\n",
      "Epoch [4/10], Step [300/1563], Loss: 1.1714\n",
      "Epoch [4/10], Step [400/1563], Loss: 1.1779\n",
      "Epoch [4/10], Step [500/1563], Loss: 1.1721\n",
      "Epoch [4/10], Step [600/1563], Loss: 1.2038\n",
      "Epoch [4/10], Step [700/1563], Loss: 1.1760\n",
      "Epoch [4/10], Step [800/1563], Loss: 1.1804\n",
      "Epoch [4/10], Step [900/1563], Loss: 1.1549\n",
      "Epoch [4/10], Step [1000/1563], Loss: 1.1705\n",
      "Epoch [4/10], Step [1100/1563], Loss: 1.2202\n",
      "Epoch [4/10], Step [1200/1563], Loss: 1.1089\n",
      "Epoch [4/10], Step [1300/1563], Loss: 1.1283\n",
      "Epoch [4/10], Step [1400/1563], Loss: 1.1305\n",
      "Epoch [4/10], Step [1500/1563], Loss: 1.1655\n",
      "Epoch [4/10], Accuracy on test set: 59.95%\n",
      "Epoch [5/10], Step [100/1563], Loss: 1.1091\n",
      "Epoch [5/10], Step [200/1563], Loss: 1.1252\n",
      "Epoch [5/10], Step [300/1563], Loss: 1.1264\n",
      "Epoch [5/10], Step [400/1563], Loss: 1.1520\n",
      "Epoch [5/10], Step [500/1563], Loss: 1.1249\n",
      "Epoch [5/10], Step [600/1563], Loss: 1.1761\n",
      "Epoch [5/10], Step [700/1563], Loss: 1.1495\n",
      "Epoch [5/10], Step [800/1563], Loss: 1.1625\n",
      "Epoch [5/10], Step [900/1563], Loss: 1.1187\n",
      "Epoch [5/10], Step [1000/1563], Loss: 1.1320\n",
      "Epoch [5/10], Step [1100/1563], Loss: 1.1423\n",
      "Epoch [5/10], Step [1200/1563], Loss: 1.1082\n",
      "Epoch [5/10], Step [1300/1563], Loss: 1.1347\n",
      "Epoch [5/10], Step [1400/1563], Loss: 1.0952\n",
      "Epoch [5/10], Step [1500/1563], Loss: 1.0951\n",
      "Epoch [5/10], Accuracy on test set: 60.24%\n",
      "Epoch [6/10], Step [100/1563], Loss: 1.0863\n",
      "Epoch [6/10], Step [200/1563], Loss: 1.0846\n",
      "Epoch [6/10], Step [300/1563], Loss: 1.0994\n",
      "Epoch [6/10], Step [400/1563], Loss: 1.0810\n",
      "Epoch [6/10], Step [500/1563], Loss: 1.0720\n",
      "Epoch [6/10], Step [600/1563], Loss: 1.0975\n",
      "Epoch [6/10], Step [700/1563], Loss: 1.1536\n",
      "Epoch [6/10], Step [800/1563], Loss: 1.1126\n",
      "Epoch [6/10], Step [900/1563], Loss: 1.0811\n",
      "Epoch [6/10], Step [1000/1563], Loss: 1.1056\n",
      "Epoch [6/10], Step [1100/1563], Loss: 1.1427\n",
      "Epoch [6/10], Step [1200/1563], Loss: 1.0747\n",
      "Epoch [6/10], Step [1300/1563], Loss: 1.0972\n",
      "Epoch [6/10], Step [1400/1563], Loss: 1.1143\n",
      "Epoch [6/10], Step [1500/1563], Loss: 1.0876\n",
      "Epoch [6/10], Accuracy on test set: 61.35%\n",
      "Epoch [7/10], Step [100/1563], Loss: 1.0975\n",
      "Epoch [7/10], Step [200/1563], Loss: 1.0877\n",
      "Epoch [7/10], Step [300/1563], Loss: 1.0387\n",
      "Epoch [7/10], Step [400/1563], Loss: 1.0591\n",
      "Epoch [7/10], Step [500/1563], Loss: 1.0712\n",
      "Epoch [7/10], Step [600/1563], Loss: 1.1047\n",
      "Epoch [7/10], Step [700/1563], Loss: 1.0691\n",
      "Epoch [7/10], Step [800/1563], Loss: 1.0850\n",
      "Epoch [7/10], Step [900/1563], Loss: 1.0518\n",
      "Epoch [7/10], Step [1000/1563], Loss: 1.1013\n",
      "Epoch [7/10], Step [1100/1563], Loss: 1.0713\n",
      "Epoch [7/10], Step [1200/1563], Loss: 1.0795\n",
      "Epoch [7/10], Step [1300/1563], Loss: 1.0627\n",
      "Epoch [7/10], Step [1400/1563], Loss: 1.0703\n",
      "Epoch [7/10], Step [1500/1563], Loss: 1.0489\n",
      "Epoch [7/10], Accuracy on test set: 61.39%\n",
      "Epoch [8/10], Step [100/1563], Loss: 1.0560\n",
      "Epoch [8/10], Step [200/1563], Loss: 1.0582\n",
      "Epoch [8/10], Step [300/1563], Loss: 1.0483\n",
      "Epoch [8/10], Step [400/1563], Loss: 1.0551\n",
      "Epoch [8/10], Step [500/1563], Loss: 1.0295\n",
      "Epoch [8/10], Step [600/1563], Loss: 1.0292\n",
      "Epoch [8/10], Step [700/1563], Loss: 1.0671\n",
      "Epoch [8/10], Step [800/1563], Loss: 1.0628\n",
      "Epoch [8/10], Step [900/1563], Loss: 1.0478\n",
      "Epoch [8/10], Step [1000/1563], Loss: 1.0565\n",
      "Epoch [8/10], Step [1100/1563], Loss: 1.0764\n",
      "Epoch [8/10], Step [1200/1563], Loss: 1.0461\n",
      "Epoch [8/10], Step [1300/1563], Loss: 1.0631\n",
      "Epoch [8/10], Step [1400/1563], Loss: 1.0466\n",
      "Epoch [8/10], Step [1500/1563], Loss: 1.0545\n",
      "Epoch [8/10], Accuracy on test set: 62.25%\n",
      "Epoch [9/10], Step [100/1563], Loss: 1.0273\n",
      "Epoch [9/10], Step [200/1563], Loss: 1.0572\n",
      "Epoch [9/10], Step [300/1563], Loss: 1.0503\n",
      "Epoch [9/10], Step [400/1563], Loss: 1.0610\n",
      "Epoch [9/10], Step [500/1563], Loss: 1.0462\n",
      "Epoch [9/10], Step [600/1563], Loss: 1.0175\n",
      "Epoch [9/10], Step [700/1563], Loss: 1.0497\n",
      "Epoch [9/10], Step [800/1563], Loss: 1.0782\n",
      "Epoch [9/10], Step [900/1563], Loss: 1.0355\n",
      "Epoch [9/10], Step [1000/1563], Loss: 1.0397\n",
      "Epoch [9/10], Step [1100/1563], Loss: 1.0565\n",
      "Epoch [9/10], Step [1200/1563], Loss: 1.0312\n",
      "Epoch [9/10], Step [1300/1563], Loss: 1.0217\n",
      "Epoch [9/10], Step [1400/1563], Loss: 1.0155\n",
      "Epoch [9/10], Step [1500/1563], Loss: 1.0190\n",
      "Epoch [9/10], Accuracy on test set: 62.47%\n",
      "Epoch [10/10], Step [100/1563], Loss: 1.0357\n",
      "Epoch [10/10], Step [200/1563], Loss: 1.0735\n",
      "Epoch [10/10], Step [300/1563], Loss: 1.0129\n",
      "Epoch [10/10], Step [400/1563], Loss: 1.0242\n",
      "Epoch [10/10], Step [500/1563], Loss: 1.0067\n",
      "Epoch [10/10], Step [600/1563], Loss: 1.0082\n",
      "Epoch [10/10], Step [700/1563], Loss: 0.9899\n",
      "Epoch [10/10], Step [800/1563], Loss: 1.0474\n",
      "Epoch [10/10], Step [900/1563], Loss: 1.0324\n",
      "Epoch [10/10], Step [1000/1563], Loss: 1.0206\n",
      "Epoch [10/10], Step [1100/1563], Loss: 0.9975\n",
      "Epoch [10/10], Step [1200/1563], Loss: 1.0359\n",
      "Epoch [10/10], Step [1300/1563], Loss: 1.0342\n",
      "Epoch [10/10], Step [1400/1563], Loss: 1.0178\n",
      "Epoch [10/10], Step [1500/1563], Loss: 1.0426\n",
      "Epoch [10/10], Accuracy on test set: 64.06%\n"
     ]
    }
   ],
   "source": [
    "from trainer import Trainer\n",
    "ModelTrainer = Trainer(cnn_model, cifar_train_loader, cifar_validation_loader, 10, 0.001)\n",
    "ModelTrainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/test_32x32.mat\n",
      "256\n",
      "tensor([   2,    0,    0, 1000, 1000, 1000, 1000, 1000,    5,    6,    6, 1000,\n",
      "           9, 1000,    1, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
      "        1000,    1, 1000,    9,    9, 1000, 1000,    4, 1000,    8, 1000,    8,\n",
      "        1000, 1000, 1000,    8,    5, 1000, 1000, 1000, 1000, 1000, 1000,    6,\n",
      "        1000, 1000,    4, 1000, 1000, 1000,    1, 1000,    8,    0, 1000, 1000,\n",
      "        1000, 1000, 1000,    1,    3, 1000,    8, 1000,    5,    0, 1000, 1000,\n",
      "        1000, 1000, 1000, 1000, 1000, 1000,    3,    6,    1, 1000, 1000, 1000,\n",
      "           0, 1000, 1000, 1000,    7, 1000, 1000,    6, 1000, 1000, 1000,    6,\n",
      "        1000, 1000, 1000, 1000,    5, 1000, 1000, 1000, 1000, 1000,    7, 1000,\n",
      "        1000, 1000,    6, 1000,    1, 1000, 1000, 1000, 1000,    0,    5, 1000,\n",
      "        1000, 1000, 1000, 1000, 1000,    4,    1, 1000, 1000, 1000, 1000, 1000,\n",
      "        1000, 1000, 1000, 1000,    6, 1000, 1000, 1000,    7, 1000, 1000,    9,\n",
      "        1000, 1000, 1000,    7, 1000,    8, 1000, 1000,    7, 1000,    8, 1000,\n",
      "        1000, 1000,    3, 1000, 1000,    5, 1000, 1000, 1000, 1000, 1000, 1000,\n",
      "        1000, 1000, 1000, 1000,    1, 1000,    5, 1000, 1000, 1000, 1000, 1000,\n",
      "           8,    4, 1000, 1000, 1000, 1000, 1000, 1000, 1000,    2, 1000,    6,\n",
      "           3, 1000, 1000, 1000, 1000,    7, 1000, 1000, 1000,    2, 1000, 1000,\n",
      "        1000, 1000, 1000, 1000,    7, 1000, 1000, 1000,    3, 1000, 1000, 1000,\n",
      "        1000, 1000,    7, 1000,    8,    3, 1000, 1000, 1000, 1000, 1000, 1000,\n",
      "        1000, 1000, 1000,    7, 1000,    0,    4, 1000,    9, 1000,    0,    0,\n",
      "        1000, 1000, 1000,    4, 1000, 1000, 1000, 1000, 1000,    9, 1000,    2,\n",
      "           2, 1000, 1000, 1000])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "def set_to_1000(_):\n",
    "    return 1000\n",
    "\n",
    "def set_to_1(_):    \n",
    "    return 1\n",
    "# OOD dataset (e.g. SVHN)\n",
    "ood_dataset = datasets.SVHN(root=\"data\", split=\"test\", transform=validation_transform, download=True, target_transform=set_to_1000)\n",
    "id_dataset = CIFAR10(root=\"./data\", train=True, transform=train_transform, download=True, target_transform=set_to_1)\n",
    "ood_loader = DataLoader(ood_dataset, batch_size=32, shuffle=False)\n",
    "loader = DataLoader(ood_dataset + validation_dataset, batch_size=32, shuffle = True)\n",
    "\n",
    "for images, labels in loader:\n",
    "    print(len(labels))\n",
    "    print(labels)  # Transformed labels (e.g., some are 1000)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data must contain ID and OOD samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m         metrics\u001b[38;5;241m.\u001b[39mupdate(detector(inputs), \u001b[38;5;28mid\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 5. Now compute the metrics (AUROC, etc.)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_ood/utils/metrics.py:275\u001b[0m, in \u001b[0;36mOODMetrics.compute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    272\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    273\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 275\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_ood/utils/metrics.py:229\u001b[0m, in \u001b[0;36mOODMetrics._compute\u001b[0;34m(self, labels, scores)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# there must now be ID and OOD samples\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(torch\u001b[38;5;241m.\u001b[39munique(labels)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData must contain ID and OOD samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    231\u001b[0m scores, scores_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msort(scores, stable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    232\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels[scores_idx]\n",
      "\u001b[0;31mValueError\u001b[0m: Data must contain ID and OOD samples."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_ood.utils import OODMetrics\n",
    "from pytorch_ood.detector import OpenMax\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Wrap your trained model with OpenMax\n",
    "detector = OpenMax(cnn_model.to(device))\n",
    "cnn_model.eval()\n",
    "\n",
    "# 2. Create OOD metrics collector\n",
    "metrics = OODMetrics()\n",
    "\n",
    "# 3. Fit OpenMax on in-distribution (ID) data\n",
    "detector.fit(cifar_validation_loader)  # Make sure this loader has ID samples\n",
    "\n",
    "# 4. Evaluate both ID and OOD\n",
    "with torch.no_grad():\n",
    "    # ---- OOD pass (label=True for each sample) ----\n",
    "    for inputs, id in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        metrics.update(detector(inputs), id)\n",
    "\n",
    "# 5. Now compute the metrics (AUROC, etc.)\n",
    "results = metrics.compute()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
